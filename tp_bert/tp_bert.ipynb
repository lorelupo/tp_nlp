{"cells":[{"cell_type":"markdown","metadata":{"id":"VYYjrbod7F3V"},"source":["# TP BERT\n","\n"]},{"cell_type":"markdown","metadata":{"id":"BJX8UgsoZMtQ"},"source":["## Introduction"]},{"cell_type":"markdown","metadata":{"id":"drnebPZ26_yA"},"source":["Dans ce TP nous allons:\n","0. Comprendre comment utiliser un *Jupyter Notebook*, un outil très populaire en analyse des données\n","1. Préparer un jeu de données pour l’apprentissage d’un modèle [BERT](https://arxiv.org/abs/1810.04805)\n","3. Comprendre comment utiliser un modèle BERT pré-entraîné\n","4. Créer un modèle de classification en classes multiples qui exploite les représentations cachés d’un encodeur BERT\n","5. Entraîner ce modèle et tester ses performances\n","\n","\n","Avec les outils suivants:\n","\n","1. [PyTorch](https://pytorch.org/docs/stable/index.html) : une bibliothèque Python *open-source* pour l'apprentissage machine. Si vous avez des difficultées avec Pytorch, il est conseillé de jeter un œil aux tutoriels: https://pytorch.org/tutorials/ . Étant donné l'importance de Pytorch, et puisque nous l'utiliserons également dans un autre TP, vous verrez que le temps consacré à lire les tutoriels aura été profitable.\n","2. [HuggingFace’s Transformers](https://huggingface.co/transformers/) : une bibliothèque basée sur Pytorch pour le traitement automatique des langues et notamment les modèles neuronaux de type Transformer (comme BERT)\n","3. [HuggingFace’s Tokenizers](https://github.com/huggingface/tokenizers): une bibliothèque basée sur Pytorch pour la tokenisation, explicitement conçue pour travailler avec la bibliothèque Transformers\n","4. Google Colab, qui héberge ce *Jupyter Notebook*. Avant de commencer le TP, vous pouvez consulter des pages d'introductions [à Colab](https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l01c01_introduction_to_colab_and_python.ipynb#scrollTo=YHI3vyhv5p85) et [aux Notebooks](https://realpython.com/jupyter-notebook-introduction/)\n","\n","\n","Les machines Colab sont livrées avec un système Linux et un environnement Python avec plusieurs bibliothèques pré-installées comme, par exemple, Pytorch. Néanmoins, si vous êtes sur une machine Colab, il faut d'abord installer les bibliothèques de HuggingFace, qui ne sont pas pré-installées.\n","\n","Pour exécuter une commande Unix sur un Jupyter Notebook, il faut placer un point d’exclamation avant la commande: `!commande`. "]},{"cell_type":"code","metadata":{"id":"OvdKhj2zlq03"},"source":["!pip install transformers tokenizers"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eknnVYcJT7-g"},"source":["Maintenant nous pouvons importer les bibliothéques principales dont nous aurons besoin. "]},{"cell_type":"code","metadata":{"id":"RQfvlh9lllN-"},"source":["import torch\n","import transformers\n","\n","# Managing arrays\n","import numpy as np\n","\n","# Plotting tools:\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","# load the TensorBoard notebook extension\n","%load_ext tensorboard"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t9aD8i7_XGe9"},"source":["**GPU**\n","\n","L'apprentissage de notre réseau de neurones requiert beaucoup de calculs matriciels. Pour exécuter ces calculs plus rapidement, il est possible d'utiliser un processeur graphique (*GPU*) p. Si vous êtes sur Colab, vous pouvez utiliser un *GPU* en sélectionnant _Runtime -> Change runtime type -> GPU_."]},{"cell_type":"code","metadata":{"id":"egeCpaPTEPRA"},"source":["if torch.cuda.is_available():\n","  print(\"GPU is available.\")\n","  device = torch.cuda.current_device()\n","else:\n","  print(\"Will work on CPU.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GBr8HR8WizRt"},"source":["### À rendre\n","\n","Chaque exercice de ce TP demande une réponse sous forme textuelle ou sous forme de code. Toute réponse doit être écrite dans une ou plusieurs cellules après l’énoncé de chaque exercice.\n","\n","Vous rendrez un répertoire compressé `tp_bert_nom1_nom2.zip` avec le contenu du répertoire `tp_bert.zip` dont le fichier `tp_bert.ipynb` aura été mis à jour avec vos reponses."]},{"cell_type":"markdown","metadata":{"id":"3itFE_0tZIBv"},"source":["## Données"]},{"cell_type":"markdown","metadata":{"id":"sbHy5DlHllOf"},"source":["\n","\n","Nous allons entraîner notre modèle sur une tache de **classification en classes multiples**. Notamment, la tâche est celle de repartir des textes en trois catégories de sentiments:\n","\n","1. négatif\n","2. neutre\n","3. positif\n","\n","Ces donnés sont collectées dans le jeu de données [FinancialPhraseBank-v1.0\n","](https://www.researchgate.net/publication/251231364_FinancialPhraseBank-v10), que vous pouvez trouver dans le répertoire de ce TP.\n","\n","Voici les informations essentielles à la compréhension de la tâche, qui figurent dans le file README.txt :\n","\n","---\n","\n","<em>The key arguments for the low utilization of statistical techniques in financial sentiment analysis have been the difficulty of implementation for practical applications and the lack of high quality training data for building such models. Especially in the case of finance and economic texts, annotated collections are a scarce resource and many are reserved for proprietary use only. To resolve the missing training data problem, we present a collection of ∼ 5000 sentences to establish human-annotated standards for benchmarking alternative modeling techniques. \n","\n","<em>The objective of the phrase level annotation task was to classify each example sentence into a positive, negative or neutral category by considering only the information explicitly available in the given sentence. Since the study is focused only on financial and economic domains, the annotators were asked to consider the sentences from the view point of an investor only; i.e. whether the news may have positive, negative or neutral influence on the stock price. As a result, sentences which have a sentiment that is not relevant from an economic or financial perspective are considered neutral.\n","\n","<em>This release of the financial phrase bank covers a collection of 4840 sentences. The selected collection of phrases was annotated by 16 people with adequate background knowledge on financial markets. Three of the annotators were researchers and the remaining 13 annotators were master’s students at Aalto University School of Business with majors primarily in finance, accounting, and economics.\n","\n","<em>Given the large number of overlapping annotations (5 to 8 annotations per sentence), there are several ways to define a majority vote based gold standard. To provide an objective comparison, we have formed 4 alternative reference datasets based on the strength of majority agreement: \n","\n","1. sentences with 100% agreement [file=Sentences_AllAgree.txt]; \n","2. sentences with more than 75% agreement [file=Sentences_75Agree.txt]; \n","3. sentences with more than 66% agreement [file=Sentences_66Agree.txt]; and \n","4. sentences with more than 50% agreement [file=Sentences_50Agree.txt].\n","\n","<em>All reference datasets are included in the release. The files are in a machine-readable \"@\"-separated format:\n","\n","<em>**sentence@sentiment**\n","\n","<em>where sentiment is either \"positive, neutral or negative\".\n","\n","<em>E.g.,  The operating margin came down to 2.4 % from 5.7 % .@negative<em>\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"2-Xmt1rCMZTp"},"source":["#### Exercice 1\n","\n","Nous allons utiliser les phrases contenues dans _Sentences_75Agree.txt_ pour entraîner et tester notre modèle. Pour ce faire, téléchargez le fichier sur la machine Colab (utiliser l’interface à gauche) ou, si vous travaillez sur un _Jupyter Notebook_ en local, placez le fichier _Sentences_75Agree.txt_ dans le même répertoire du _Notebook_.\n","\n","Ensuite, écrivez la fonction `load_data()` qui lit les phrases contenues dans ce fichier et les sépare de leurs étiquettes. Le résultat de cette fonction, assigné à la variable `data`, doit être une liste de listes, où chaque sous-liste contient une phrase comme premier élément et son étiquette comme deuxième élément. I.e.:\n","\n","````\n","data[0][0] == 'A high court in Finland has fined seven local asphalt companies more than   lion ( $ 117 million ) for operating a cartel .'\n","data[0][1] == 'negative'\n","````\n","\n","> Note: l'encodage du fichier _Sentences_75Agree.txt_ est `iso-8859-1`."]},{"cell_type":"code","metadata":{"id":"_RUyKYiCPrUT"},"source":["# WRITE FUNCTION load_data HERE\n","\n","filename = 'Sentences_75Agree.txt'\n","classes = ['negative', 'neutral', 'positive']\n","data = load_data(filename, classes)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Gdz1-MmsO72_"},"source":["Testez le résultat de votre fonction en faisant tourner la cellule suivante."]},{"cell_type":"code","metadata":{"id":"Ws2gU_wEOp8S"},"source":["assert type(data[0][0]) == str, \"The first element of every sub-list should be a sentence.\"\n","assert data[0][1] in classes, \"The second element of each sub-list should belong to one of the three classes available.\"\n","assert len(data) == 3453, \"The size of data should be of 3453 sentences.\"\n","print(\"Test passed!\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jJQ_PsuiQpcp"},"source":["Maintenant, nous allons séparer nos données pour l’entraînement afin de tester notre modèle."]},{"cell_type":"code","metadata":{"id":"UI8nt4K09PBA"},"source":["import random, math\n","from sklearn import preprocessing\n","\n","# randomly sample train and test set from data\n","random.seed(0)\n","random.shuffle(data)\n","split = 0.85\n","num_elem = math.floor(len(data)*split)\n","train = data[:num_elem]\n","test = data[num_elem:]\n","print(\"Train data size : \", len(train))\n","print(\"Test data size : \", len(test))\n","\n","# build input features and labels\n","x_train = [t[0] for t in train]\n","y_train = [t[1] for t in train]\n","x_test = [t[0] for t in test]\n","y_test = [t[1] for t in test]\n","\n","# convert labels into integers ['negative', 'neutral', 'positive'] -> [0, 1, 2]\n","le = preprocessing.LabelEncoder()\n","le.fit(y_train)\n","y_train = le.transform(y_train)\n","y_test = le.transform(y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RETCPekGaoKX"},"source":["### Baseline\n"]},{"cell_type":"markdown","metadata":{"id":"ypuFm78gllPM"},"source":["#### Exercice 2\n","\n","Préalablement à une tache de classification, c’est toujours une bonne idée de vérifier si les données de training et de test sont (plus ou moins) uniformément distribuées par rapport aux classes existantes.\n","\n","1. Pourquoi ?\n","\n","2. Quel est le nombre de données d'entraînement et de test correspondant à chaque classe ?\n","\n","3. Étant donnée la distribution des étiquettes dans le _testset_, quelle sont les attentes par rapport aux performances de notre modèle en terme de _accuracy_ ? "]},{"cell_type":"code","metadata":{"id":"BqQUhkuyllPO"},"source":["# WRITE CODE TO ANSWER THE QUESTIONS HERE,  PRINT ANSWERS OR WRITE THEM IN A TEXT CELL BELOW"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0fknJN5vWqlO"},"source":["#### Exercice 3\n","\n","Dans la cellule ci-dessous, on entraîne un modèle de [classification naïve bayésienne\n"," en classes multiples](https://fr.wikipedia.org/wiki/Classification_na%C3%AFve_bay%C3%A9sienne), en sorte d’avoir une performance de référence. Ainsi, l’objectif dans la suite sera de faire mieux de ce naïf.\n","\n","1. Pourquoi est-qu’on s’attende qu’un modèle de classification type BERT est plus puissante du modèle naïf bayésien ?\n","2. Dans quelles conditions les modèles non-neuronaux peuvent être plus avantageux ?"]},{"cell_type":"code","metadata":{"id":"1326MzvnWtPU"},"source":["from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.model_selection import cross_val_score\n","\n","# transform sentences in bag of words\n","count_vect = CountVectorizer()\n","x_train_counts = count_vect.fit_transform(x_train)\n","x_test_counts = count_vect.transform(x_test)\n","# transform bag of words in tfidf vectors\n","tfidf_transformer = TfidfTransformer()\n","x_train_tfidf = tfidf_transformer.fit_transform(x_train_counts)\n","x_test_tfidf = tfidf_transformer.fit_transform(x_test_counts)\n","# train a multinomial Naive-Bayes classifier on the tfidf vectors\n","classifier = MultinomialNB().fit(x_train_tfidf, y_train)\n","# use classifier to predict the labels of the test set\n","predicted = classifier.predict(x_test_tfidf)\n","# calculate accuracy\n","print(\"Accuracy of a simple multinomial Naive-Bayes approach :\", sum(predicted == y_test)/len(predicted))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uhPQ-kYZN0Ev"},"source":["## Tokenisation"]},{"cell_type":"markdown","metadata":{"id":"-vf3eq1vllQK"},"source":["La bibliothèque HuggingFace offre des outils pour faire la tokenisation des données textuelles selon les modéles que nous allons utiliser: [see the docs](https://huggingface.co/transformers/tokenizer_summary.html).\n","\n","Dans ce TP, nous allons utiliser un modèles pré-entraîné qui s’appelle [DistilBERT](https://arxiv.org/pdf/1910.01108.pdf): un encodeur avec une architecture [Transformer](https://arxiv.org/abs/1706.03762) une version distillée de BERT et qui ainsi plus léger en terme de mémoire et plus rapide. Les auteurs le présentent dans leur papier de la manière suivante :\n","\n","<em>As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study."]},{"cell_type":"code","metadata":{"id":"QQlbOyzHllQN"},"source":["from transformers import DistilBertTokenizer\n","\n","MAX_LEN = 512\n","\n","tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', padding=True, truncation=True)\n","\n","# let's check out how the tokenizer works\n","for n in range(3):\n","    # tokenize sentences\n","    tokenizer_out = tokenizer(x_train[n])\n","    # convert numerical tokens to alphabetical tokens\n","    encoded_tok = tokenizer.convert_ids_to_tokens(tokenizer_out.input_ids)\n","    # decode tokens back to string\n","    decoded = tokenizer.decode(tokenizer_out.input_ids)\n","    print(tokenizer_out)\n","    print(encoded_tok, '\\n')\n","    print(decoded, '\\n')\n","    print('---------------- \\n')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mT5wpgrUllQZ"},"source":["#### Exercice 4\n","\n","1. Qu’est-ce que la sortie du tokeniseur (`tokeniser_out`) représente ? \n","2. Quels sont les tokens spéciaux introduits par le tokeniseur? Quel est leur fonction ?\n","3. Pourquoi certains tokens commencent par ## (par exemple, ##rea ##der) ? Quel intérêt ?\n","4. Notez que le tokeniseur que l'on utilise a été \"entraîné\". Pourquoi un tokeniseur comme celui-ci nécessite d’un entraînement préalable à son application ? **Aide**: creuser le sujet Byte Pair Encodings."]},{"cell_type":"markdown","metadata":{"id":"_42B63m-llQp"},"source":["#### Exercise 5\n","\n","BERT (et DistilBERT) gère des séquences de longueur maximale égale à 512 (`MAX_LEN=512`). Vérifier si cette longueur est optimale pour notre tâche. En d'autre termes, vérifiez quelle est la distribution des longueurs des textes que nous devons classifier et s’il serait avantageux de réduire MAX_LEN de façon à réduire le temps de traitement des séquences. "]},{"cell_type":"code","metadata":{"id":"abnwQh5KllQs"},"source":["# WRITE CODE TO ANSWER THE QUESTION HERE.  PRINT THE ANSWER OR WRITE IT IN A TEXT CELL BELOW"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9Zpe2gwqiN0P"},"source":["Maintenant que nous avons compris comment le tokeniseur fonctionne, nous pouvons désormais écrire une classe Dataset qui nous servira pour l'entraînement et le test. En effet, le classes Python qui s’occupent de la création de _batches_ et du training nécessitent en entrée une classe de type Dataset, comme la suivante.\n","\n","**Note** changez MAX_LEN si vous avez trouvé dans l'exercice précédent une valeur qui est plus avantageuse en terme de temps de calcul."]},{"cell_type":"code","metadata":{"id":"HFI7JYSbllSk"},"source":["from torch.utils.data import Dataset, DataLoader\n","\n","# MAX_LEN =\n","\n","class MyDataset(Dataset):\n","    def __init__(self, sentences, labels, tokenizer, max_len):\n","        # variables that are set when the class is instantiated\n","        self.sentences = sentences\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.sentences)\n","  \n","    def __getitem__(self, item):\n","        # select the sentence and its class\n","        sentence = str(self.sentences[item])\n","        label = self.labels[item]\n","        # tokenize the sencence\n","        tokenizer_out = self.tokenizer(\n","            sentence,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            return_token_type_ids=False,\n","            pad_to_max_length=True,\n","            return_attention_mask=True,\n","            return_tensors='pt',\n","            truncation=True\n","            )\n","        # return a dictionary with the output of the tokenizer and the label\n","        return  {\n","            'input_ids': tokenizer_out['input_ids'].flatten(),\n","            'attention_mask': tokenizer_out['attention_mask'].flatten(),\n","            'label': torch.tensor(label, dtype=torch.long)\n","        }\n","\n","\n","# instantiate two MyDataset objects\n","train_dataset = MyDataset(x_train, y_train, tokenizer, MAX_LEN)\n","test_dataset = MyDataset(x_test, y_test, tokenizer, MAX_LEN)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x8Eyuxv5KKlh"},"source":["## Modèle"]},{"cell_type":"markdown","metadata":{"id":"f-0mxvMC00YP"},"source":["C’est le moment de comprendre comment DistilBERT fonctionne. Hugginface nous offre plusieurs modèles pré-entraînes de type DistilBERT, nous allons utiliser [distilbert-base-uncased](https://huggingface.co/distilbert-base-uncased), qui a une architecture profonde de 66’362’880 paramétres et qui a été pré-entraîné sur le jeu de données BookCorpus pour 90 heures avec huit GPUs de 16 GB de mémoire. Cela nous permettra d’obtenir de très bons résultat en attachant un classificateur très léger (un seul layer) au top de DistilBERT, qui fera le plus gros du travail, c’est-à-dire l’encodage de nos textes. Notre classificateur nécessitera d’être entraîné pendant quelques minutes seulement. \n","\n","On peut trouver les noms de tous les autres modèles pré-entraînés offerts par HuggingFace [à cette addresse](https://huggingface.co/models). \n","\n","Pour télécharger un modèle pré-entraîné on utilise l'option `.from_pretrained`:"]},{"cell_type":"code","metadata":{"id":"gKUjMtVrDx2D"},"source":["from transformers import DistilBertModel\n","\n","PRE_TRAINED_MODEL_NAME = 'distilbert-base-uncased'\n","\n","distilbert = DistilBertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PS5JItniHzP0"},"source":["Nous pouvons encoder les séquences de textes avec une passe en avant de distilBERT, qui nous renvoie les représentations cachés de la dernière couche.\n","\n","**Note** : la dimension des couches de `distilbert-base-uncased` est de 768 neurones \n","\n","**Note2** : nous utilisons la fonction `.unsqueeze(0)` parce que normalement on passe au modèle des batches, alors que cette fois ci ce n’est qu'un seul élement de la classe *MyDataset*. Avec `.unsqueeze(0)`, nous le traitons comme un batch de taille 1."]},{"cell_type":"code","metadata":{"id":"A_aq1_vqE3kK"},"source":["first_sent = train_dataset[0]\n","\n","hidden_state = distilbert(\n","    input_ids=first_sent['input_ids'].unsqueeze(0), attention_mask=first_sent['attention_mask'].unsqueeze(0)\n","    )\n","\n","hidden_state[0].shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gxZwaorkI11b"},"source":["distilbert.config"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SpNwIOQUKdHj"},"source":["#### Exercice 6\n","\n","Nous avons maintenant les éléments nécessaires à la construction d'un modèle de classification en classes multiples basé sur DistilBERT. \n","\n","Nous allons définire une classe `DistilBertForSentimentClassification` pour notre modèle. Cette classe importe la classe DistilBertPreTrainedModel, qui a son tour importe la classe Pytorch nn.Module. Comme vous pouvez le voir dans la documentation Pytorch, la classe Module définie une méthode `forward()`, qu'on essaye de remplacer dans la définition de `DistilBertForSentimentClassification`. La méthode `forward()` définie le comportement du modèle lorsqu'on lui donne des données en entrée. Par exemple, vous pouvez considerer la commande `encoder_output = self.encoder(...input...)` (voir code ci-dessous) comme équivalente à `encoder_output = self.encoder.forward(...input...)` , parce que `self.encoder` est une instance de la classe `DistilBertModel`, qui est aussi basée sur `nn.Module` (voir documentation/code HuggingFace pour plus de détails).\n","    \n","\n","Concernant le code ci-dessous :\n","\n","1. Quelle est la fonction de cet extrait de code ? Quel est l'intérêt?\n","\t```Python\n","\tif freeze_encoder:\n","\t\tfor param in self.encoder.parameters():\n","\t\t\tparam.requires_grad = False\n","\t```\n","\n","2. Pourquoi on ne garde que la représentation cachée (vecteur encodé) du premier token de chaque séquence avec la commande `pooled_output = hidden_state[:, 0, :]` ?\n","\n","    - **aide**: Pour comprendre pourquoi on peut ne garder que l'encodage du premier token de chaque phrase, il faut essayer de comprendre quelle information est rapresentée par ce vecteur. Pour ce faire, il est important de comprendre la méthode d'entraînement des modèles type BERT, où le token `[CLS]` à un role très spécifique. Vous trouverez les informations nécessaires à comprendre cela dans le paper [BERT](https://arxiv.org/pdf/1810.04805.pdf). Si vous trouvez le papier difficile, vous pouvez aussi trouver des explications simplifiées et résumées sur youtube/medium/etc.\n","\n","3. Compléter l’extrait de code sous-jacente à `if labels is not None:` de façon à calculer la fonction de perte (*loss  function*) du modèle lorsque les cibles sont passées à la fonction `forward`\n","\n","    - **aide**: Vous n'avez qu'à écrire une ligne de code: `loss = torch.nn.functional.???(???,???)` en choisissant la bonne fonction de perte. Les fonctions de perte implémentées en Pytorch sont répertoriées ici: https://pytorch.org/docs/stable/nn.html#loss-functions. Vous trouvez en ligne les informations pour choisir la fonction de perte la plus adaptée à la tache. Par exemple, en tapant \"pytorch loss functions\" sur Google, on trouve ces deux articles: https://neptune.ai/blog/pytorch-loss-functions et https://medium.com/udacity-pytorch-challengers/a-brief-overview-of-loss-functions-in-pytorch-c0ddb78068f7 .\n","\n"]},{"cell_type":"code","metadata":{"id":"IPqk1pk6llTM"},"source":["from transformers import DistilBertPreTrainedModel, DistilBertConfig\n","\n","\n","PRE_TRAINED_MODEL_NAME = 'distilbert-base-uncased'\n","FREEZE_PRETRAINED_MODEL = True\n","\n","class DistilBertForSentimentClassification(DistilBertPreTrainedModel):\n","    def __init__(self, config, num_labels, freeze_encoder=False):\n","        # instantiate the parent class DistilBertPreTrainedModel\n","        super().__init__(config)\n","        # instantiate num. of classes\n","        self.num_labels = num_labels\n","        # instantiate and load a pretrained DistilBERT model as encoder\n","        self.encoder = DistilBertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n","        # [Q1] freeze the encoder parameters if required \n","        if freeze_encoder:\n","          for param in self.encoder.parameters():\n","              param.requires_grad = False\n","        # the classifier: a feed-forward layer attached to the encoder's head\n","        self.classifier = torch.nn.Linear(\n","            in_features=config.dim, out_features=self.num_labels, bias=True)\n","        # instantiate a dropout function for the classifier's input\n","        self.dropout = torch.nn.Dropout(p=0.1)\n","\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        labels=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","    ):\n","        # encode a batch of sequences with DistilBERT\n","        encoder_output = self.encoder(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            head_mask=head_mask,\n","            inputs_embeds=inputs_embeds,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","        )\n","        # extract the hidden representations from the encoder output\n","        hidden_state = encoder_output[0]  # (bs, seq_len, dim)\n","        # only select the encoding corresponding to the first token\n","        # of each sequence in the batch [Q2]\n","        pooled_output = hidden_state[:, 0, :]  # (bs, dim)\n","        # apply dropout\n","        pooled_output = self.dropout(pooled_output)  # (bs, dim)\n","        # feed into the classifier\n","        logits = self.classifier(pooled_output)  # (bs, dim)\n","\n","        outputs = (logits,) + encoder_output[1:]\n","        \n","        if labels is not None:\n","          # [Q3] COMPLETE CODE HERE\n","          # loss = ...\n","\n","          # aggregate outputs\n","          outputs = (loss,) + outputs\n","\n","        return outputs  # (loss), logits, (hidden_states), (attentions)\n","\n","\n","# instantiate model\n","model = DistilBertForSentimentClassification(\n","    config=distilbert.config, num_labels=len(classes),\n","    freeze_encoder = FREEZE_PRETRAINED_MODEL\n","    )\n","\n","# print info about model's parameters\n","total_params = sum(p.numel() for p in model.parameters())\n","model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n","trainable_params = sum([np.prod(p.size()) for p in model_parameters])\n","print('model total params: ', total_params)\n","print('model trainable params: ', trainable_params)\n","print('\\n', model)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I1bjajujllTv"},"source":["## Entraînement"]},{"cell_type":"markdown","metadata":{"id":"kfrOtIN4NSaK"},"source":["*Let's train our classifier!*\n","\n","Le moment est arrivé. Heureusement *HuggingFace* nous fournit la classe Python nécessaire à gérer le training : `Trainer`. Nous lui passons les hyper-paramètres avec la classe `TrainingArgument`. Nous allons aussi lui passer les métrique que nous désirons pour l’évaluation du modèle en phase de test :\n","\n","1. justesse (accuracy)\n","2. précision (precision)\n","3. rappel (recall)\n","4. f1\n"]},{"cell_type":"code","metadata":{"id":"0W8S-cFL_rsU"},"source":["# clean logs and results directory from old files\n","# this could be useful if we were running the cells below multiple times\n","!rm -r ./logs ./results"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oIsuEFzillTx"},"source":["from transformers import Trainer, TrainingArguments\n","from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n","\n","def compute_metrics(pred):\n","    labels = pred.label_ids\n","    preds = pred.predictions.argmax(-1)\n","    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds)\n","    acc = accuracy_score(labels, preds)\n","    return {\n","        'accuracy': acc,\n","        'f1': f1,\n","        'precision': precision,\n","        'recall': recall\n","    }\n","\n","training_args = TrainingArguments(\n","    output_dir='./results',          \n","    logging_dir='./logs',\n","    logging_first_step=True,\n","    logging_steps=50,\n","    num_train_epochs=16,              \n","    per_device_train_batch_size=8,  \n","    learning_rate=5e-5,\n","    weight_decay=0.01        \n",")\n","\n","trainer = Trainer(\n","    model=model,                         \n","    args=training_args,                  \n","    train_dataset=train_dataset,         \n","    compute_metrics=compute_metrics\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O_HknicPPbhA"},"source":["Pour visualiser l’évolution de la *loss* tout au long de l'entraînement, nous pouvons utiliser *TensorBoard *:"]},{"cell_type":"code","metadata":{"id":"1BZrFuTg81Sh"},"source":["%tensorboard --logdir logs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V4NW8wLAllT7"},"source":["train_results = trainer.train()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U9j6ZBcQllUV"},"source":["test_results = trainer.predict(test_dataset=test_dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"72_BIMte5Q4K"},"source":["print('Predictions: \\n', test_results.predictions)\n","print('\\nAccuracy: ', test_results.metrics['eval_accuracy'])\n","print('Precision: ', test_results.metrics['eval_precision'])\n","print('Recall: ', test_results.metrics['eval_recall'])\n","print('F1: ', test_results.metrics['eval_f1'])\n","print(classes)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HL1A5EzIPzxT"},"source":["Si nous avons fait les choses bien, la performance (*accuracy*) de notre modèle dans la classification des phraces qu’il n’a jamais vu est d'environ 70.0%. Cela n'est pas très satisfaisant si on considère :\n","\n","1. que la distribution des classes dans notre ensemble de test n’était pas équilibrée (c.f. Exercice 2)\n","2. qu'un modèle de classification naïve bayésienne obtient une accuracy similaire, voir supérieure (c.f. Exercice 3)\n"," "]},{"cell_type":"markdown","metadata":{"id":"T21FtfBO5TBG"},"source":["### Améliorer les performances"]},{"cell_type":"markdown","metadata":{"id":"UIiPrE74SRV6"},"source":["#### Exercice 7\n","\n","1. Comment est-ce qu'on peut améliorer le résultat du modèle ?\n","2. Ré-entraînez le modèle avec la nouvelle stratégie et calculez ses performances avec les métriques utilisées avant. Quelle _accuracy_ vous arrivez à obtenir ?\n","\n","**Aide** : quels hyperparamétres influencent l'entrainement du modèle ? Rappelez vous aussi que nous avons congelé l'encodeur."]},{"cell_type":"code","metadata":{"id":"ssy9fAXU2w4S"},"source":["# WRITE CODE TO ANSWER THE QUESTION HERE.  PRINT THE ANSWER OR WRITE IT IN A TEXT CELL BELOW"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TMSR2zq-cbp3"},"source":["Si nous somme satisfaits de la performance de notre modèle, nous pouvons le sauvegarder pour l’utiliser dans le prochain exercice : "]},{"cell_type":"code","metadata":{"id":"FrOnJarvBDf2"},"source":["MODEL_PATH = './my_model'\n","trainer_best.save_model(MODEL_PATH)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t8MUjNgvllUg"},"source":["## Prédictions"]},{"cell_type":"markdown","metadata":{"id":"sX0bwUA5RkST"},"source":["#### Exercice 8\n","\n","En utilisant le modèle que l’on vient de sauvegarder, prédire la classe à laquelle appartient les phrases suivantes :\n","\n","````\n","  \"CocaCola saw its share price dropping of more than 25% this semester.\",\n","  \"Despite most of the company's sales are taking place in China, the shareholders decided not to relocate the production there.\" ,\n","  \"This year's profits quintuplicated with respect to the last year.\"\n","````\n","\n","1. Quelles classes a prédit le modèle ?\n","2. Avec quelle probabilité ?"]},{"cell_type":"code","metadata":{"id":"4ZXfWabRllUh"},"source":["# WRITE CODE TO ANSWER THE QUESTION HERE.  PRINT THE ANSWER OR WRITE IT IN A TEXT CELL BELOW"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sne9jShhiNae"},"source":["## Changer de modèle"]},{"cell_type":"markdown","metadata":{"id":"E4uf5_NyiQ41"},"source":["#### Exercice 9 (bonus)\n","\n","Expérimentez avec d’autres modèles pré-entraînés. La liste des modèles pré-entraînés est disponible ici: https://huggingface.co/models.\n","\n","- Qu’est-ce que vous avez essayé ? Laissez votre code dans des cellules ci-dessous. \n","- Qu’est-ce que vous avez appris ?"]},{"cell_type":"code","metadata":{"id":"9FVOmYuOYamk"},"source":[""],"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"name":"tp_bert.ipynb","provenance":[{"file_id":"1fwQUIP-CBqCPX7G9VsSrZCcTYBtbQ2bV","timestamp":1597325044260},{"file_id":"1yCAZissLcIfQwi3w7LmcbECY2Na_uJC1","timestamp":1596802372921},{"file_id":"1tYxlWekCCnDqxCOtNILc--bFaUyGIDuq","timestamp":1596784786868}],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}